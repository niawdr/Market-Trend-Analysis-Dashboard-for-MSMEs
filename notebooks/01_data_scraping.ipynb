{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Environment VS Code"
      ],
      "metadata": {
        "id": "hWBmKURPy_Qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "version 1"
      ],
      "metadata": {
        "id": "E0GvjOvOgGZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6n69mGXHIru",
        "outputId": "f754fda8-20b2-4711-bf6f-740ab60b9d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1: https://www.tokopedia.com/p/makanan-minuman/makanan-ringan/kacang?page=1\n",
            "Request failed: HTTPSConnectionPool(host='www.tokopedia.com', port=443): Read timed out. (read timeout=10)\n",
            "Request failed: HTTPSConnectionPool(host='www.tokopedia.com', port=443): Read timed out. (read timeout=10)\n",
            "Request failed: HTTPSConnectionPool(host='www.tokopedia.com', port=443): Read timed out. (read timeout=10)\n",
            "Failed to fetch page 1.\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "import time, random\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Setup WebDriver\n",
        "driver = webdriver.Chrome()\n",
        "url = \"https://www.tokopedia.com/p/makanan-minuman/makanan-ringan/camilan-instant\"\n",
        "driver.get(url)\n",
        "\n",
        "data = []\n",
        "max_halaman = 50\n",
        "\n",
        "for halaman in range(max_halaman):\n",
        "    print(f\"\\nüîÑ Memproses halaman ke-{halaman + 1}\")\n",
        "    try:\n",
        "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#zeus-root\")))\n",
        "        time.sleep(random.uniform(2, 4))\n",
        "\n",
        "        for _ in range(17):\n",
        "            driver.execute_script(\"window.scrollBy(0, 250)\")\n",
        "            time.sleep(0.7)\n",
        "\n",
        "        driver.execute_script(\"window.scrollBy(50, 0)\")\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        product_links = set()\n",
        "\n",
        "        for item in soup.findAll('div', class_='css-bk6tzz e1nlzfl2'):\n",
        "            link_tag = item.find('a', href=True)\n",
        "            if link_tag:\n",
        "                url_produk = link_tag['href']\n",
        "                if 'ta.tokopedia.com' not in url_produk and '/product/' not in url_produk:\n",
        "                    if not url_produk.startswith('https://'):\n",
        "                        url_produk = f\"https://www.tokopedia.com{url_produk}\"\n",
        "                    product_links.add(url_produk)\n",
        "\n",
        "        print(f\"üõí Ditemukan {len(product_links)} produk.\")\n",
        "\n",
        "        for product_link in product_links:\n",
        "            try:\n",
        "                driver.execute_script(\"window.open(arguments[0]);\", product_link)\n",
        "                driver.switch_to.window(driver.window_handles[1])\n",
        "                time.sleep(random.uniform(3, 5))\n",
        "\n",
        "                detail_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "                def get_text(selector):\n",
        "                    try:\n",
        "                        return selector.text.strip()\n",
        "                    except:\n",
        "                        return ''\n",
        "\n",
        "                product_name = get_text(detail_soup.find('h1', class_='css-j63za0'))\n",
        "                price = get_text(detail_soup.find('div', class_='price', attrs={'data-testid': 'lblPDPDetailProductPrice'}))\n",
        "                rating_product = get_text(detail_soup.find('span', {'data-testid': 'lblPDPDetailProductRatingNumber'}))\n",
        "                review_counts = get_text(detail_soup.find('span', {'data-testid': 'lblPDPDetailProductRatingCounter'}))\n",
        "                sold_count = get_text(detail_soup.find('p', {'data-testid': 'lblPDPDetailProductSoldCounter'})).replace(\"Terjual \", \"\")\n",
        "\n",
        "                # kategori\n",
        "                breadcrumb_items = driver.find_elements(By.CSS_SELECTOR, 'ol[data-testid=\"lnkPDPDetailBreadcrumb\"] li')\n",
        "                # ambil kategori ke-4 (index 3) yang merupakan kategori \"Kacang\"\n",
        "                if len(breadcrumb_items) >= 4:\n",
        "                    category = breadcrumb_items[3].text.strip()  # kategori ke-4 (index 3)\n",
        "                else:\n",
        "                    category = ''\n",
        "\n",
        "                # informasi toko\n",
        "                store_name = get_text(detail_soup.find('h2', class_='css-nc7wd7-unf-heading'))\n",
        "                store_loc_raw = get_text(detail_soup.find('h2', class_='css-g78l6p-unf-heading'))\n",
        "                store_loc = store_loc_raw.replace(\"Dikirim dari \", \"\")\n",
        "\n",
        "                try:\n",
        "                    rating_review_store = detail_soup.find('div', class_='css-e39d2g').find('p').text.strip()\n",
        "                    rating_store = rating_review_store.split(\" \")[0]\n",
        "                    review_count_store = rating_review_store.split(\" \")[1].strip(\"()\")\n",
        "                except:\n",
        "                    rating_store = ''\n",
        "                    review_count_store = ''\n",
        "\n",
        "                # Rating 1-5 Count\n",
        "                rating_distribution = {}\n",
        "                rating_elements = detail_soup.find_all('span', class_='css-myjxhx')\n",
        "                for i, span in enumerate(rating_elements):\n",
        "                    rating_distribution[f'Rating {i+1} Count'] = span.text.strip().replace('(', '').replace(')', '')\n",
        "\n",
        "                data.append({\n",
        "                    'Product Name': product_name,\n",
        "                    'Price': price,\n",
        "                    'Product Rating': rating_product,\n",
        "                    'Rating 1 Count': rating_distribution.get('Rating 1 Count', ''),\n",
        "                    'Rating 2 Count': rating_distribution.get('Rating 2 Count', ''),\n",
        "                    'Rating 3 Count': rating_distribution.get('Rating 3 Count', ''),\n",
        "                    'Rating 4 Count': rating_distribution.get('Rating 4 Count', ''),\n",
        "                    'Rating 5 Count': rating_distribution.get('Rating 5 Count', ''),\n",
        "                    'Product Reviews Count': review_counts,\n",
        "                    'Sold Count': sold_count,\n",
        "                    'Category': category,\n",
        "                    'Store Name': store_name,\n",
        "                    'Store Location': store_loc,\n",
        "                    'Store Rating': rating_store,\n",
        "                    'Store Reviews Count': review_count_store,\n",
        "                    'URL': product_link\n",
        "                })\n",
        "\n",
        "                print(f\"‚úÖ {product_name}\")\n",
        "                driver.close()\n",
        "                driver.switch_to.window(driver.window_handles[0])\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Gagal ambil detail dari {product_link} karena {e}\")\n",
        "                if len(driver.window_handles) > 1:\n",
        "                    driver.close()\n",
        "                    driver.switch_to.window(driver.window_handles[0])\n",
        "                continue\n",
        "\n",
        "        # Klik ke halaman berikutnya\n",
        "        try:\n",
        "            next_button = WebDriverWait(driver, 10).until(\n",
        "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.css-dzvl4q-unf-pagination-item[aria-label='Laman berikutnya']\"))\n",
        "            )\n",
        "            if not next_button.is_enabled():\n",
        "                print(\"‚ö†Ô∏è Tombol 'Laman berikutnya' tidak aktif. Berhenti.\")\n",
        "                break\n",
        "\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
        "            time.sleep(1)\n",
        "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
        "            print(\"‚û°Ô∏è Klik laman berikutnya berhasil.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Tidak bisa klik laman berikutnya: {e}\")\n",
        "            break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error pada halaman ke-{halaman+1}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Simpan ke csv lokal\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "folder_path = r\"D:\\TA\\Data Tokopedia\"\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "csv_path = os.path.join(folder_path, \"Data_Tokopedia_Camilan-instant.csv\")\n",
        "\n",
        "df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\n‚úÖ Data berhasil disimpan ke: {csv_path}\")\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "version 2"
      ],
      "metadata": {
        "id": "rBMnGDLTgLYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "import time, random\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Setup WebDriver\n",
        "driver = webdriver.Chrome()\n",
        "url = \"https://www.tokopedia.com/p/makanan-minuman/makanan-ringan/camilan-instant\"\n",
        "driver.get(url)\n",
        "\n",
        "data = []\n",
        "max_halaman = 50\n",
        "\n",
        "for halaman in range(max_halaman):\n",
        "    print(f\"\\nüîÑ Memproses halaman ke-{halaman + 1}\")\n",
        "    try:\n",
        "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#zeus-root\")))\n",
        "        time.sleep(random.uniform(2, 4))\n",
        "\n",
        "        for _ in range(17):\n",
        "            driver.execute_script(\"window.scrollBy(0, 250)\")\n",
        "            time.sleep(0.7)\n",
        "\n",
        "        driver.execute_script(\"window.scrollBy(50, 0)\")\n",
        "        time.sleep(1)\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        product_links = set()\n",
        "\n",
        "        for item in soup.findAll('div', class_='css-bk6tzz e1nlzfl2'):\n",
        "            link_tag = item.find('a', href=True)\n",
        "            if link_tag:\n",
        "                url_produk = link_tag['href']\n",
        "                if 'ta.tokopedia.com' not in url_produk and '/product/' not in url_produk:\n",
        "                    if not url_produk.startswith('https://'):\n",
        "                        url_produk = f\"https://www.tokopedia.com{url_produk}\"\n",
        "                    product_links.add(url_produk)\n",
        "\n",
        "        print(f\"üõí Ditemukan {len(product_links)} produk.\")\n",
        "\n",
        "        for product_link in product_links:\n",
        "            try:\n",
        "                driver.execute_script(\"window.open(arguments[0]);\", product_link)\n",
        "                driver.switch_to.window(driver.window_handles[1])\n",
        "                time.sleep(random.uniform(3, 5))\n",
        "\n",
        "                detail_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "                def get_text(selector):\n",
        "                    try:\n",
        "                        return selector.text.strip()\n",
        "                    except:\n",
        "                        return ''\n",
        "\n",
        "                # Nama produk\n",
        "                product_name = get_text(detail_soup.find('h1', class_='css-j63za0'))\n",
        "\n",
        "                # Harga produk\n",
        "                price = get_text(detail_soup.find('div', class_='price', attrs={'data-testid': 'lblPDPDetailProductPrice'}))\n",
        "\n",
        "                # Rating produk\n",
        "                rating_product = get_text(detail_soup.find('span', {'data-testid': 'lblPDPDetailProductRatingNumber'}))\n",
        "\n",
        "                # Jumlah ulasan produk\n",
        "                review_counts = get_text(detail_soup.find('span', {'data-testid': 'lblPDPDetailProductRatingCounter'}))\n",
        "\n",
        "                # Jumlah produk terjual\n",
        "                sold_count = get_text(detail_soup.find('p', {'data-testid': 'lblPDPDetailProductSoldCounter'})).replace(\"Terjual \", \"\")\n",
        "\n",
        "                # Kategori produk\n",
        "                breadcrumb_items = driver.find_elements(By.CSS_SELECTOR, 'ol[data-testid=\"lnkPDPDetailBreadcrumb\"] li')\n",
        "                if len(breadcrumb_items) >= 4:\n",
        "                    category = breadcrumb_items[3].text.strip()\n",
        "                else:\n",
        "                    category = ''\n",
        "\n",
        "                # Nama toko\n",
        "                store_name = get_text(detail_soup.find('h2', class_='css-nc7wd7-unf-heading'))\n",
        "\n",
        "                # Lokasi toko\n",
        "                store_loc_raw = get_text(detail_soup.find('h2', class_='css-g78l6p-unf-heading'))\n",
        "                store_loc = store_loc_raw.replace(\"Dikirim dari \", \"\")\n",
        "\n",
        "                try:\n",
        "                    rating_review_store = detail_soup.find('div', class_='css-e39d2g').find('p').text.strip()\n",
        "                    # Rating toko\n",
        "                    rating_store = rating_review_store.split(\" \")[0]\n",
        "\n",
        "                    # Jumlah ulasan toko\n",
        "                    review_count_store = rating_review_store.split(\" \")[1].strip(\"()\")\n",
        "                except:\n",
        "                    rating_store = ''\n",
        "                    review_count_store = ''\n",
        "\n",
        "                # Distribusi jumlah rating produk skala 1-5\n",
        "                rating_distribution = {}\n",
        "                rating_elements = detail_soup.find_all('span', class_='css-myjxhx')\n",
        "                for i, span in enumerate(rating_elements):\n",
        "                    rating_distribution[f'Rating {i+1} Count'] = span.text.strip().replace('(', '').replace(')', '')\n",
        "\n",
        "                data.append({\n",
        "                    'Product Name': product_name,\n",
        "                    'Price': price,\n",
        "                    'Product Rating': rating_product,\n",
        "                    'Rating 1 Count': rating_distribution.get('Rating 1 Count', ''),\n",
        "                    'Rating 2 Count': rating_distribution.get('Rating 2 Count', ''),\n",
        "                    'Rating 3 Count': rating_distribution.get('Rating 3 Count', ''),\n",
        "                    'Rating 4 Count': rating_distribution.get('Rating 4 Count', ''),\n",
        "                    'Rating 5 Count': rating_distribution.get('Rating 5 Count', ''),\n",
        "                    'Product Reviews Count': review_counts,\n",
        "                    'Sold Count': sold_count,\n",
        "                    'Category': category,\n",
        "                    'Store Name': store_name,\n",
        "                    'Store Location': store_loc,\n",
        "                    'Store Rating': rating_store,\n",
        "                    'Store Reviews Count': review_count_store,\n",
        "                    'URL': product_link\n",
        "                })\n",
        "\n",
        "                print(f\"‚úÖ {product_name}\")\n",
        "                driver.close()\n",
        "                driver.switch_to.window(driver.window_handles[0])\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Gagal ambil detail dari {product_link} karena {e}\")\n",
        "                if len(driver.window_handles) > 1:\n",
        "                    driver.close()\n",
        "                    driver.switch_to.window(driver.window_handles[0])\n",
        "                continue\n",
        "\n",
        "        # Navigasi ke halaman berikutnya\n",
        "        try:\n",
        "            next_button = WebDriverWait(driver, 10).until(\n",
        "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.css-dzvl4q-unf-pagination-item[aria-label='Laman berikutnya']\"))\n",
        "            )\n",
        "            if not next_button.is_enabled():\n",
        "                print(\"‚ö†Ô∏è Tombol 'Laman berikutnya' tidak aktif. Berhenti.\")\n",
        "                break\n",
        "\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
        "            time.sleep(1)\n",
        "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
        "            print(\"‚û°Ô∏è Klik laman berikutnya berhasil.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Tidak bisa klik laman berikutnya: {e}\")\n",
        "            break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error pada halaman ke-{halaman+1}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Simpan ke file csv lokal\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "folder_path = r\"D:\\TA\\Data Tokopedia\"\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "csv_path = os.path.join(folder_path, \"Data_Tokopedia_Camilan-instant.csv\")\n",
        "\n",
        "df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"\\n‚úÖ Data berhasil disimpan ke: {csv_path}\")\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WecxctayR9u2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}